{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip install -U tokenizers","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tokenizers\n  Downloading tokenizers-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 3.5 MB/s eta 0:00:01\n\u001b[31mERROR: transformers 2.11.0 has requirement tokenizers==0.7.0, but you'll have tokenizers 0.8.0 which is incompatible.\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.7.0\n    Uninstalling tokenizers-0.7.0:\n      Successfully uninstalled tokenizers-0.7.0\nSuccessfully installed tokenizers-0.8.0\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.15","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting tensorflow==1.15\n  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n\u001b[K     |████████████████████████████████| 412.3 MB 18 kB/s s eta 0:00:01    |███████▏                        | 92.1 MB 45.2 MB/s eta 0:00:08\n\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.29.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.14.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.12.2)\nCollecting gast==0.2.2\n  Downloading gast-0.2.2.tar.gz (10 kB)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\nCollecting keras-applications>=1.0.8\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 5.2 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.18.1)\nCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nCollecting tensorboard<1.16.0,>=1.15.0\n  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 42.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.11.2)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.34.2)\nCollecting tensorflow-estimator==1.15.1\n  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n\u001b[K     |████████████████████████████████| 503 kB 49.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.9.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.2.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (46.1.3.post20200325)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.1)\nBuilding wheels for collected packages: gast\n  Building wheel for gast (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=7005ccfacec2c50a000be4e44ed2ffec47452996212fd23eb8bceeb4f066a8c7\n  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\nSuccessfully built gast\n\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: keras 2.4.0 has requirement tensorflow>=2.2.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\nInstalling collected packages: gast, keras-applications, astor, tensorboard, tensorflow-estimator, tensorflow\n  Attempting uninstall: gast\n    Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.2.2\n    Uninstalling tensorboard-2.2.2:\n      Successfully uninstalled tensorboard-2.2.2\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.2.0\n    Uninstalling tensorflow-estimator-2.2.0:\n      Successfully uninstalled tensorflow-estimator-2.2.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.2.0\n    Uninstalling tensorflow-2.2.0:\n      Successfully uninstalled tensorflow-2.2.0\nSuccessfully installed astor-0.8.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tokenizers","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt = tokenizers.BertWordPieceTokenizer(\n    vocab_file=None,\n    unk_token='[UNK]',\n    sep_token='[SEP]',\n    cls_token='[CLS]',\n    clean_text=True,\n    handle_chinese_chars=True,\n    strip_accents=True,\n    lowercase=True,\n    wordpieces_prefix='##'\n)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt.train(\n    files=[\"../input/hindi-oscar-corpus/hi_dedup_1000.txt\"],\n    vocab_size=30000,\n    min_frequency=3,\n    limit_alphabet=1000,\n    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwpt.save_model(\"/kaggle/working/\", \"hindi\")","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"['/kaggle/working/hindi-vocab.txt']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ../input/bertsrc/","execution_count":24,"outputs":[{"output_type":"stream","text":"/kaggle/input/bertsrc\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":8,"outputs":[{"output_type":"stream","text":"CONTRIBUTING.md\t\t    optimization_test.py\r\nLICENSE\t\t\t    predicting_movie_reviews_with_bert_on_tf_hub.ipynb\r\nREADME.md\t\t    requirements.txt\r\n__init__.py\t\t    run_classifier.py\r\ncreate_pretraining_data.py  run_classifier_with_tfhub.py\r\nextract_features.py\t    run_pretraining.py\r\nmodeling.py\t\t    run_squad.py\r\nmodeling_test.py\t    sample_text.txt\r\nmultilingual.md\t\t    tokenization.py\r\noptimization.py\t\t    tokenization_test.py\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python create_pretraining_data.py \\\n    --input_file=/kaggle/input/hindi-oscar-corpus/hi_dedup_1000.txt \\\n    --output_file=/kaggle/working/tf_examples.tfrecord \\\n    --vocab_file=/kaggle/working/hindi-vocab.txt \\\n    --do_lower_case=True \\\n    --max_seq_length=128 \\\n    --max_predictions_per_seq=20 \\\n    --masked_lm_prob=0.15 \\\n    --random_seed=42 \\\n    --dupe_factor=5","execution_count":9,"outputs":[{"output_type":"stream","text":"W0701 23:24:19.907574 140630622037824 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n\nW0701 23:24:19.907903 140630622037824 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n\nW0701 23:24:19.908287 140630622037824 module_wrapper.py:139] From /kaggle/input/bertsrc/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\nW0701 23:24:19.943599 140630622037824 module_wrapper.py:139] From create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n\nW0701 23:24:19.945683 140630622037824 module_wrapper.py:139] From create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n\nI0701 23:24:19.945983 140630622037824 create_pretraining_data.py:446] *** Reading from input files ***\nI0701 23:24:19.946150 140630622037824 create_pretraining_data.py:448]   /kaggle/input/hindi-oscar-corpus/hi_dedup_1000.txt\nI0701 23:24:24.967849 140630622037824 create_pretraining_data.py:457] *** Writing to output files ***\nI0701 23:24:24.968171 140630622037824 create_pretraining_data.py:459]   /kaggle/working/tf_examples.tfrecord\nW0701 23:24:24.968428 140630622037824 module_wrapper.py:139] From create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n\nI0701 23:24:24.969383 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.969657 140630622037824 create_pretraining_data.py:151] tokens: [CLS] दनी होगी । आइए ##िकरिया ह बक म मिलन वाल कछ ऐस ही अधिकार [MASK] क बार म . . [MASK] बको क लिए जररी [MASK] कि दिवाली डिप [MASK] ##ज ##िट खात [MASK] की विशष शरत ##ो की जानकारी गराहको को खाता खोलन क समय द । यह किसी [MASK] गराह ##क का हक [MASK] । अगर बक [MASK] नही [MASK] रह ह [MASK] आप [SEP] मरा खडा लड दख कर मीना [MASK] उत ##ा ##वली हो [MASK] थी [MASK] उसन मझ वही घा ##स क [MASK] ढर पर ध ##क बधन द दिया . . मरा लड मीना क हाथ म इधर - उधर होन लगा और फिर लड को मसत सात ##वन ##ा [MASK] जब मीना न उस मह म कवा लिया । [SEP]\nI0701 23:24:24.969849 140630622037824 create_pretraining_data.py:161] input_ids: 2 5285 1039 136 5981 4492 128 898 120 1858 462 494 844 381 1548 3 97 460 120 18 18 3 3301 97 350 1482 3 326 6021 3276 3 249 603 2482 3 316 1479 4574 210 316 945 4996 317 2102 4041 97 606 113 136 416 536 3 3535 207 319 2328 3 136 552 898 3 357 3 379 128 3 353 4 751 2302 776 525 315 920 3 553 218 4387 328 3 524 3 977 732 850 3717 222 97 3 5272 312 114 207 5318 113 459 18 18 751 776 920 97 1551 120 4234 17 5214 558 583 322 712 776 317 5329 2357 556 218 3 504 920 115 382 438 120 3256 955 136 4\nI0701 23:24:24.970018 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.970163 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.970273 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 5 15 21 26 28 30 34 51 56 60 62 65 74 79 81 88 93 117 124 0\nI0701 23:24:24.970384 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 1689 210 18 128 124 224 210 348 128 1123 315 377 348 1896 322 97 361 1542 685 0\nI0701 23:24:24.970488 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.970577 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.971080 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.971292 140630622037824 create_pretraining_data.py:151] tokens: [CLS] me ##x ##ico & ap ##os ; s reg ##ul [MASK] [MASK] ##as ##on fin [MASK] ##e [MASK] as we ##e ##k 17 will [MASK] p ##us ##he ##d [MASK] ##ack to acc ##om [MASK] ##od ##ate the pl ##et ##hor ##a of national te ##am ##s that will take [MASK] [MASK] ##eld in the com ##ing day ##s . mo [MASK] [MASK] the pl ##ay ##o ##ff fi ##eld has be ##en set , but the ##re is st ##ill the ch ##ance we & ap ##os ; [MASK] ##l se ##e [SEP] तो [MASK] आप [MASK] चाहत ह कि गा ##न [MASK] नाम क साथ बरथड [MASK] कस करत ह वो भी [MASK] म [MASK] तो फिर [MASK] दिया गया वि ##डियो परा दख [SEP]\nI0701 23:24:24.971462 140630622037824 create_pretraining_data.py:161] input_ids: 2 1148 270 5792 10 559 985 31 60 3073 648 3 3 533 366 5133 3 239 3 1495 2468 239 283 1079 2692 3 57 737 1465 240 3 6043 787 4172 573 3 1748 493 762 1888 604 4436 260 657 1170 2122 725 244 3447 2692 4924 3 3 5754 490 762 909 549 5129 244 18 3666 3 3 762 1888 973 241 2546 2928 5754 3663 1882 375 5180 16 3226 762 547 1263 870 1968 762 840 4557 2468 10 559 985 31 3 247 651 239 4 377 3 353 3 1168 128 326 924 215 3 705 97 386 4993 3 1151 540 128 777 348 3 120 3 377 712 3 459 413 344 717 548 525 4\nI0701 23:24:24.971619 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.971762 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.971860 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 11 12 16 18 25 30 35 51 52 62 63 90 96 98 104 109 115 117 120 0\nI0701 23:24:24.971971 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 359 651 385 16 1882 43 250 762 2928 464 657 53 552 3080 120 605 3291 35 1779 0\nI0701 23:24:24.972079 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.972167 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.972629 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.972828 140630622037824 create_pretraining_data.py:151] tokens: [CLS] ##di [MASK] mar ##ty [MASK] [MASK] rs | जमम कशमीर म पाकिसतानी सना [MASK] ##पर [MASK] फिर ली भारतीय सन ##िक की [MASK] world box ##ing ch ##amp ##ions ##hi ##p mar ##y k ##om pi [MASK] ##ks you ##n ##g ##er box ##ers [MASK] bi [MASK] th ##re ##at sa ##y ##s read ##y for them s ##ks | [MASK] मक ##क [SEP] [MASK] जिसम एटी - आक ##सी [MASK] ##स भर ##पर मातरा म [MASK] जाता ह [MASK] इसकी मदद स कई बीमार [MASK] को दर भग [MASK] जा सकता ह । इस विभिनन तरह क वय ##जन करी , दाल , सब ##जिय ##ो , सम ##ोस ##ा आदि म मिला ##या जाता ह [MASK] इसम वि ##टा ##मिन , कर [MASK] ##ली [SEP]\nI0701 23:24:24.973026 140630622037824 create_pretraining_data.py:161] input_ids: 2 492 3 819 4429 3 3 3679 69 4819 3587 120 6458 957 3 407 3 712 714 979 940 323 316 3 4093 4176 549 840 5870 3901 1250 238 819 248 52 573 5167 3 2555 1374 232 265 341 4176 1358 3 2925 3 465 547 367 3681 248 244 1851 248 1120 3448 60 2555 69 3 996 207 4 3 1113 1803 17 2474 372 3 222 685 407 6272 120 3 601 128 3 1323 1430 127 704 5740 3 317 442 1816 3 351 689 128 136 329 1877 738 97 1270 590 1941 16 4290 16 449 5496 210 16 347 3333 218 1337 120 1729 313 601 128 3 1040 344 537 2336 16 315 3 365 4\nI0701 23:24:24.973176 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.973323 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.973420 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 2 5 6 14 16 23 37 45 47 61 65 70 71 77 80 86 90 118 125 0\nI0701 23:24:24.973522 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 1358 547 240 275 115 431 277 1495 4984 981 16 372 1519 3428 136 405 1402 136 1417 0\nI0701 23:24:24.973622 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.973710 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.974163 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.974378 140630622037824 create_pretraining_data.py:151] tokens: [CLS] [MASK] को [MASK] परो पर खड होन पर ही [MASK] करना चाहिए । ४ . अकल का [MASK] - ( मर ##ख ) - राम तम मरी बात [MASK] नही मानत , लगता ह आजकल तम अकल क दशमन [MASK] गए हो । ५ . अपना उलल [MASK] करना - ( मतलब निकालन ##ा ) - आजकल क नता अपना [MASK] उलल सीधा [SEP] ९ . आसमान स बात करना - ( बहत ऊचा होना ) - आजकल ऐसी ऐसी इमारत बनन लगी ह , जो आसमान स बात करती ह । [MASK] ##० . ईट स ##min [MASK] ##ाना - ( परी तरह [MASK] नषट करना [MASK] [MASK] राम चाहता था कि [MASK] अपन शतर क [MASK] की ईट स ईट बज ##ा द । [SEP]\nI0701 23:24:24.974550 140630622037824 create_pretraining_data.py:161] input_ids: 2 3 317 3 856 312 2481 558 312 381 3 563 1004 136 142 18 2472 319 3 17 12 592 227 13 17 858 1181 1102 579 3 357 3121 16 1693 128 2826 1181 2472 97 3499 3 638 328 136 143 18 783 4055 3 563 17 12 1869 2402 218 13 17 2826 97 1242 783 3 4055 4606 4 146 18 3134 127 579 563 17 12 669 3254 1222 13 17 2826 1304 1304 4925 4594 1819 128 16 441 3134 127 579 1470 128 136 3 299 18 3253 127 5501 3 497 17 12 634 738 3 3747 563 3 3 858 1961 402 326 3 369 4362 97 3 316 3253 127 3253 1814 218 113 136 4\nI0701 23:24:24.974696 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.974842 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.974969 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 1 3 10 18 29 40 48 49 61 67 86 93 98 99 105 108 109 114 118 0\nI0701 23:24:24.975070 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 210 369 3405 3499 719 328 4606 563 783 3134 441 139 3253 1814 127 13 17 555 739 0\nI0701 23:24:24.975172 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.975267 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.975710 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.975906 140630622037824 create_pretraining_data.py:151] tokens: [CLS] एक इवट पर आधारित फिलम बता दिया गया [MASK] भारत का ऑडियो गोलड जीतन का इवट । और अब [MASK] दवगन भी सपोरटस फिलम म दिलचसपी दिखा रह ह । 2018 की ईद क [MASK] रस 3 फाइनल हो चकी ह । सलमान खान की दो और फिलमो मालवा चरचा जोर ##ो पर एचडी । एक ह भारत जो कि भारत - पाकिसतान सबधो पर ताल ##लक रखत ##ी ह और ##जञ ह रम [MASK] डी [MASK] ##जा की डास फिलम जिस [SEP] शाहरख खान गिननी आनद एल राय की बौ ##न वाली फिलम कर रह ह , जिसका अभी [MASK] कोई भी नाम [MASK] तय [MASK] [MASK] ह । इसक अलावा , शाहरख क किसी परो ##जकट की कोई चरचा नही हो रही ह । [SEP]\nI0701 23:24:24.976097 140630622037824 create_pretraining_data.py:161] input_ids: 2 338 2944 312 1628 650 566 459 413 3 621 319 4243 3576 1959 319 2944 136 322 496 3 2850 348 3639 650 120 6205 2241 379 128 136 629 316 5209 97 3 2742 23 3298 328 1896 128 136 4773 744 316 479 322 2393 5955 2263 1809 210 312 2645 136 338 128 621 441 326 621 17 1258 6080 312 5277 1130 5795 230 128 322 5494 128 2025 3 1389 3 517 316 5270 650 472 4 2258 744 6331 4749 1500 1015 316 2732 215 1070 650 315 379 128 16 2214 1801 3 527 348 705 3 1646 3 3 128 136 616 1208 16 2258 97 536 856 5498 316 527 2263 357 328 567 128 136 4\nI0701 23:24:24.976252 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.976391 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.976490 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 3 9 12 14 20 21 22 35 49 54 72 75 77 86 89 101 105 107 108 0\nI0701 23:24:24.976585 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 312 136 2207 1959 1718 2850 348 350 316 128 1488 210 222 4637 1015 468 357 393 413 0\nI0701 23:24:24.976685 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.976770 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.977254 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.977454 140630622037824 create_pretraining_data.py:151] tokens: [CLS] [MASK] ##न ##पर स था [MASK] म एक यवक को सजा दन का ऐसा मामला सामन [MASK] [MASK] जिसक बाद स पलिस मह ##क [MASK] म हडकप [MASK] गया ह । अनय पिता ##om जिनह आप [MASK] ##षट [MASK] क [MASK] म ट ##ग [MASK] ह , सारवजनिक नि ##धियो [MASK] चो ##र अपन बचचो क लिए घर ##ो और सपतति छोड [MASK] ह ; यहा तक कि इस घर म हम रहत ह एक किर ##ा ##ए पर मका ##न ह . . [SEP] ब ##बस ##ाई ##ट [MASK] विषय ##वस ##सलड का सवा ##मित [MASK] , [MASK] एव सधार ##ण - अलीगढ मीडिया गरप , अलीगढ | power ##ed by qu [MASK] ##oft [SEP]\nI0701 23:24:24.977629 140630622037824 create_pretraining_data.py:161] input_ids: 2 3 215 407 127 402 3 120 338 2637 317 4371 571 319 1123 1870 1109 3 3 1690 432 127 982 438 207 3 120 5077 3 413 128 136 885 1099 573 2416 353 3 662 3 97 3 120 106 257 3 128 16 6483 721 3835 3 2706 211 369 1989 97 350 739 210 322 6340 1387 3 128 31 746 468 326 329 739 120 388 1768 128 338 2785 218 262 312 2024 215 128 18 18 4 118 2531 1159 209 3 2796 1273 4696 319 826 3355 3 16 3 809 1733 229 17 1792 1287 4709 16 1792 69 6216 599 2672 4199 3 4435 4 0 0 0 0 0 0 0 0 0 0 0\nI0701 23:24:24.977773 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\nI0701 23:24:24.977909 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\nI0701 23:24:24.978025 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 1 6 17 18 25 28 34 37 39 41 45 51 63 90 93 97 99 114 0 0\nI0701 23:24:24.978123 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 3522 215 1237 128 235 713 11 685 558 516 540 97 1341 316 236 208 5762 4622 0 0\nI0701 23:24:24.978229 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\nI0701 23:24:24.978317 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.978741 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.978935 140630622037824 create_pretraining_data.py:151] tokens: [CLS] फरास की टीम 17 ##2 ##6 अको क साथ शीरष [MASK] पहच गई ह [MASK] फरास [MASK] छह सथानो का फायदा हआ ह । फरास क अलावा फाइनल म पहचन वाली करोएशिया को सबस जयादा 16 अको का लाभ [MASK] ह [MASK] पिछल बार का चपियन जरमनी 14 सथान नीच ल ##ढ ##क ##कर [MASK] ##व नबर [MASK] खिस एटीएम गया ह । सबस जयादा [MASK] खिताब जीतन वाली अरजटीना छह [MASK] फि ##सल ##कर शीरष [MASK] 10 स [SEP] करोएशिया अब 16 ##43 अको क साथ चौथ नबर पर जॉबस गई ह । विशव कप समी ##फा ##इन ##ल तक पहची बल [MASK] ##िम मीटर सथान ऊपर उठ ##कर दसर जबकि बराजील एक सथान नीच गिर ##कर तीसर [MASK] पर [MASK] ##क गई ह । [SEP]\nI0701 23:24:24.979128 140630622037824 create_pretraining_data.py:161] input_ids: 2 2146 316 1330 1079 281 282 2694 97 386 3616 3 749 544 128 3 2146 3 3724 2865 319 2902 769 128 136 2146 97 1208 3298 120 3972 1070 2422 317 818 891 949 2694 319 1864 3 128 3 2105 460 319 6332 3614 727 1074 1779 123 263 207 340 3 208 1577 3 5230 2892 413 128 136 818 891 3 2659 1959 1070 4151 3724 3 3755 677 340 3616 3 507 127 4 2422 496 949 5541 2694 97 386 3994 1577 312 4888 544 128 136 981 766 2196 1835 1838 214 468 2397 581 3 561 2381 1074 1890 1080 340 1343 1541 4038 338 1074 1779 2486 340 4977 3 312 3 207 544 128 136 4\nI0701 23:24:24.979288 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.979426 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.979521 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 11 15 17 40 42 55 58 60 66 72 77 81 91 101 104 106 109 120 122 0\nI0701 23:24:24.979615 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 312 136 317 769 136 937 312 207 460 1074 17 2422 749 468 589 338 1080 1577 5230 0\nI0701 23:24:24.979713 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.979799 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.980245 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.980445 140630622037824 create_pretraining_data.py:151] tokens: [CLS] समारटफोन क सतर को थोडा और बढान क [MASK] विचार [MASK] [MASK] सकता ह । सवाल यह उठता ह कि कया समारटफोन म एक साथ [MASK] भाषाओ म टाइप किया जा [MASK] ह [MASK] [MASK] [MASK] यह सभव ह । तो चलिए बात करत [MASK] ऐस माधयम क बार म जिसक दवारा एडरॉयड समारटफोन पर काफी भाषा [MASK] [MASK] किया जा सकता ह । आपको बस नीच दिए गए [MASK] सट ##पस को फॉलो [MASK] [SEP] ' [MASK] सरकार दनिया की तीसरी सबस अधिक भरोसमद सरकार ' description : यह सरवकषण ऐस समय म आया ह , जब विपकषी दल [MASK] विधानसभा [MASK] परचार म जीएसटी और नोटबदी जस मददो को लकर परधानमतरी [MASK] की सरकार और भाजपा को निशाना बना रह ह । [MASK] now [SEP]\nI0701 23:24:24.980614 140630622037824 create_pretraining_data.py:161] input_ids: 2 1210 97 1244 317 6196 322 5882 97 3 1042 3 3 689 128 136 1370 416 4775 128 326 682 1210 120 338 386 3 3536 120 6110 393 351 3 128 3 3 3 416 3784 128 136 377 4704 579 540 3 844 1795 97 460 120 1690 674 3218 1210 312 770 1143 3 3 393 351 689 128 136 681 1243 1779 1342 638 3 646 1062 317 5064 3 4 11 3 498 886 316 4891 818 697 4854 498 11 3632 30 416 2423 844 606 120 1237 128 16 504 1874 1340 3 2449 3 1940 120 5087 322 4848 774 3504 317 768 1379 3 316 498 322 1211 317 2432 560 379 128 136 3 2463 4\nI0701 23:24:24.980758 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.980894 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.981010 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 9 11 12 26 32 34 35 36 45 55 58 59 70 75 78 101 103 114 125 0\nI0701 23:24:24.981122 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 350 393 351 704 689 35 860 16 128 312 120 6110 494 563 991 2627 1116 991 2629 0\nI0701 23:24:24.981255 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.981347 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.981793 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.982009 140630622037824 create_pretraining_data.py:151] tokens: [CLS] सचना का [MASK] - विभाग दवारा [MASK] 17 [MASK] ##ol ##um [MASK] समबधित पजी , कारयालय नगर पाल [MASK] निगम भ [MASK] ##ई जोन - [MASK] रिस ##ाली [SEP] . क . चनदर [MASK] ##कर [MASK] महा ##पर ##बध ##क भारत [MASK] निगम लिमिटड दरग को समपत ##तिकर की [MASK] करक [MASK] ##वारण ##ट जारी कर दिया , करक ##ी ##वारण [MASK] क साथ अध ##ि ##भार सहित राशि वस ##ल किय जान की नोटिस ता ##मि ##ल की गई ह । राषटरपति , राजयपाल [MASK] मखयमतरी , जस सवधानिक पदो पर आज महिलाए दश क विकास क [MASK] निरणय ल रही ह [MASK] परसा कोल बलॉक ##स म अडानी दवारा धनबल का दर ##प ##योग : कमपनी [MASK] रिपोरट फरजी भम ##ात ##मक और [MASK] [SEP]\nI0701 23:24:24.982187 140630622037824 create_pretraining_data.py:161] input_ids: 2 1623 319 3 17 2108 674 3 1079 3 900 2043 3 4148 2319 16 3898 5293 3288 3 5013 119 3 237 4273 17 3 2743 975 4 18 97 18 2957 3 340 3 1307 407 782 207 621 3 5013 6337 3443 317 5608 4051 316 3 1014 3 5644 209 1388 315 459 16 1014 230 5644 3 97 386 475 212 2185 3999 1682 1453 214 2563 431 316 4847 969 3354 214 316 544 128 136 1878 16 4768 3 1260 16 774 6467 3754 312 502 3997 659 97 1144 97 3 3463 123 567 128 3 5547 3391 5790 222 120 6263 674 6090 319 442 223 611 30 4541 3 1712 3983 1912 447 2536 322 3 4\nI0701 23:24:24.982340 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.982476 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.982573 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 3 7 9 12 19 22 26 34 36 42 50 52 62 86 99 104 108 119 126 0\nI0701 23:24:24.982668 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 1548 1505 44 232 323 454 1381 218 115 3322 1682 230 209 16 350 136 6466 316 104 0\nI0701 23:24:24.982767 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.982855 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.983297 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.983500 140630622037824 create_pretraining_data.py:151] tokens: [CLS] 1 . सबस पहल आपको अपन एडरॉयड डिवाइस [MASK] sw ##if ##t ##ke ##ya ##p ##p को डाउनलोड करक इसट ##ॉल [MASK] की जररत ह । अगर आपक डिवाइस म [MASK] स ही ऐप भागी ह तो [MASK] सट [MASK] को सकी ##प करक [MASK] बढ । [SEP] सल ' क नाम [MASK] भी मशहर ह . इस जल की शर ##वात स ही करत कदियो क आतमहतया ##ओ की [MASK] आती [MASK] , जो बतान क लिए [MASK] थी [MASK] यहा उनक ##ो किस तरह त ##ड [MASK] ##या जाता होगा . साल 19 ##99 म 100 स जयादा कदियो न जल म ##ोतरी कर ली थी . [MASK] आतमहतया ##ओ क कारण कछ लोग य भी [MASK] ह कि [MASK] म भ ##त - [SEP]\nI0701 23:24:24.983680 140630622037824 create_pretraining_data.py:161] input_ids: 2 21 18 818 635 681 369 3218 3211 3 5175 5457 234 1752 482 238 238 317 2109 1014 5581 852 3 316 2101 128 136 552 932 3211 120 3 127 381 1720 5693 128 377 3 646 3 317 3881 223 1014 3 767 136 4 1057 11 97 705 3 348 3153 128 18 329 692 316 428 5617 127 381 540 1984 97 6324 261 316 3 2697 3 16 441 2974 97 350 3 524 3 746 1003 210 1677 738 111 219 3 313 601 837 18 686 630 2041 120 1543 127 891 1984 115 692 120 3800 315 714 524 18 3 6324 261 97 963 494 978 121 348 3 128 326 3 120 119 236 17 4\nI0701 23:24:24.983825 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.983979 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.984082 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 9 22 31 35 38 40 45 53 65 71 73 79 81 89 105 110 117 119 122 0\nI0701 23:24:24.984178 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 120 397 635 2091 329 223 1053 127 746 1265 524 770 326 723 6324 658 121 1619 692 0\nI0701 23:24:24.984328 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.984415 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.984853 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.985062 140630622037824 create_pretraining_data.py:151] tokens: [CLS] जल ऐसी ह , जिनकी कलपना स बड [MASK] बडा अपरा ##धी भी [MASK] ##प उठता ह . कहा जाता [MASK] कि इनम स कछ जल तो [MASK] ह , जहा कदी एक दसर को [MASK] च ##बा जात ह . [MASK] जानत ह [MASK] क इन 7 खतरनाक [MASK] क बार म [MASK] स जिदा लौट आना होता ह बहद मशकिल . ( image source [MASK] source : you len [SEP] सीग का नश म [MASK] भाई बहन अजलि न मर दोसत [MASK] और मी - सकस सकस क साथ पारटी क बाद बकवास किया भारतीय सकस » hindi xxx » सीग [MASK] नश म चचर भाई बहन अजलि न [MASK] दोसत तरिगट और मी - सकस सकस [MASK] साथ पारटी क बाद बकवास किया [SEP]\nI0701 23:24:24.985249 140630622037824 create_pretraining_data.py:161] input_ids: 2 692 1304 128 16 5963 4659 127 1007 3 1507 3094 836 348 3 223 4775 128 18 426 601 3 326 2843 127 494 692 377 3 128 16 1023 2702 338 1343 317 3 101 562 1473 128 18 3 1689 128 3 97 658 27 4973 3 97 460 120 3 127 3183 4112 3251 701 128 1956 2619 18 12 2630 2267 3 2267 30 1374 5155 4 2822 319 2726 120 3 1613 2071 2872 115 592 867 3 322 530 17 881 881 97 386 1093 97 432 2609 393 979 881 75 889 5191 75 2822 3 2726 120 2709 1613 2071 2872 115 3 867 2818 322 530 17 881 881 3 386 1093 97 432 2609 393 4\nI0701 23:24:24.985402 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.985546 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.985646 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 9 14 21 27 28 36 42 45 50 54 64 67 71 77 84 85 104 112 120 0\nI0701 23:24:24.985740 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 127 319 128 377 1304 3183 5979 886 3272 1023 12 30 2466 2709 2818 322 319 592 97 0\nI0701 23:24:24.985840 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.985928 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.986408 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.986618 140630622037824 create_pretraining_data.py:151] tokens: [CLS] विशष [MASK] स अपन विरोधी [MASK] [MASK] दखकर [MASK] का अनभव करत ह । [MASK] ##ति का वचन ह जो सवारथ रह ##ित होकर [MASK] हित ##ो का परि ##त ##या ##ग करक नि [MASK] सवारथ और निष ##काम भाव स दसरो की सवा को अपना ##लिए धरम समझत [MASK] , उतत ##म परष कहलात ह । जो दसरो क भल क साथ [SEP] [MASK] टरन हादसा am ##rit ##s ##ar tr ##ain acc ##ide ##n ##t पजाब क अमतसर पर एक बडा रल हादसा हआ ह । पठानकोट स अमतसर [MASK] रही एक टरन बनाकर चपट म आन स लगभग परती लोगो की मौत हो गई ह , जबकि कई लोगो की हालात नाजक बताई जा [MASK] [MASK] । यह हादसा अमतसर म जोडा [MASK] [SEP]\nI0701 23:24:24.986797 140630622037824 create_pretraining_data.py:161] input_ids: 2 1479 3 127 369 2883 3 3 3090 3 319 2206 540 128 136 3 358 319 5356 128 441 6202 379 342 2194 3 4380 210 319 539 236 313 257 1014 721 3 6202 322 4710 3064 2583 127 2408 316 826 317 783 1195 1024 5974 3 16 2225 235 2556 6355 128 136 441 2408 97 1579 97 386 4 3 1971 2908 2923 5687 244 359 1498 3846 4172 1615 232 234 4059 97 3101 312 338 1507 1394 2908 769 128 136 6425 127 3101 3 567 338 1971 3096 5249 120 910 127 2224 3382 663 316 1976 328 544 128 16 1541 704 663 316 2103 5875 3472 351 3 3 136 416 2908 3101 120 5685 3 4\nI0701 23:24:24.986943 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.987100 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.987198 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 2 6 7 9 15 16 25 35 40 47 50 65 68 92 96 102 118 119 126 0\nI0701 23:24:24.987301 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 516 317 5743 1190 721 358 369 30 2583 1979 128 3101 2923 85 316 2667 567 128 4313 0\nI0701 23:24:24.987401 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.987497 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.987922 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.988136 140630622037824 create_pretraining_data.py:151] tokens: [CLS] scor ##ch ##ing काननी आय te [MASK] ##age ##r [MASK] ##xy emma mae takes एक विशाल [MASK] ##ro ##bb [MASK] [MASK] ##क डी ##प म उसकी टाइ ##ट tw ##at yo ##b ##t 05 : 00 [SEP] क पातर हो सकत ह , परनत हमारा आचरण इसक विपरीत हो रहा ह [MASK] हम अपन कष ##टो स तो दखी होत ह , परनत दसरो को , [MASK] [MASK] [MASK] अपन अछि को दखी दखकर सख का अनभव करत ह । [MASK] ##ति का वचन ह जो सवारथ रह ##ित होकर अपन हित ##आईपी का परि ##त ##या ##ग करक [MASK] : सवारथ [MASK] निष ##काम भाव स दसरो की सवा को अपना मखय धरम समझत ह [MASK] उतत ##म [MASK] कहलात ह । जो दसरो क [SEP]\nI0701 23:24:24.988312 140630622037824 create_pretraining_data.py:161] input_ids: 2 6046 476 549 4838 993 2122 3 1169 251 3 5524 3634 3481 6218 338 3940 3 1318 5491 3 3 207 1389 223 120 962 1810 209 2688 367 807 246 234 1561 30 4156 4 97 3072 328 564 128 16 3384 2202 6292 616 6006 328 542 128 3 388 369 586 2517 127 377 5743 1322 128 16 3384 2408 317 16 3 3 3 369 2658 317 5743 3090 1190 319 2206 540 128 136 3 358 319 5356 128 441 6202 379 342 2194 369 4380 4766 319 539 236 313 257 1014 3 30 6202 3 4710 3064 2583 127 2408 316 826 317 783 1979 1024 5974 128 3 2225 235 3 6355 128 136 441 2408 97 4\nI0701 23:24:24.988455 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.988592 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.988688 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 7 10 13 17 20 21 24 52 67 68 69 71 81 93 100 103 112 117 120 0\nI0701 23:24:24.988783 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 375 1884 3481 465 549 1305 223 136 1479 516 127 2883 721 210 721 322 783 16 2556 0\nI0701 23:24:24.988881 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.989000 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.989458 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.989661 140630622037824 create_pretraining_data.py:151] tokens: [CLS] क [MASK] एन ##आर ##आई अब दसरो को द सक ##ग ##इज , लोकसभा म जन ##पर ##ति ##नि ##धित ##व [MASK] बिल पास - [MASK] - परथम दवितीय ततीय चतर ##थी [MASK] ##मी स ##षटि सपत ##मी अस [MASK] ##ी [MASK] ##मी [MASK] ##ाइ एक ##ाद ##सी दवा [MASK] तर ##यो ##दसी चतर ##दसी पच ##दसी [MASK] ##िमा अम ##ा ##वस ##या [SEP] ##र ’ , ‘ गि ##दध ##दव [MASK] ##ली ’ , ‘ पारटी ’ , ‘ मिर ##च मसाला ’ [MASK] ‘ करम ##यो ##दधा ’ , ‘ दरो ##ह ##काल ’ , [MASK] कषण [MASK] [MASK] [MASK] ‘ मा ##च ##िस ’ , ‘ घातक ’ , [MASK] गप ##त ’ , [MASK] आसथा ’ , ‘ चा ##ची 42 ##0 [SEP]\nI0701 23:24:24.989845 140630622037824 create_pretraining_data.py:161] input_ids: 2 97 3 1570 2345 1036 496 2408 317 113 364 257 3376 16 2276 120 554 407 358 2035 2773 208 3 1909 884 17 3 17 2349 3508 5279 2137 2178 3 457 127 3500 4373 457 1440 3 230 3 457 3 409 338 343 372 2721 3 439 337 5423 2137 5423 3287 5423 3 3335 570 218 1273 313 4 211 150 16 149 4254 673 2523 3 365 150 16 149 1093 150 16 149 4331 221 3569 150 3 149 1469 337 2846 150 16 149 5286 217 3329 150 16 3 1368 3 3 3 149 535 221 352 150 16 149 6414 150 16 3 5232 236 150 16 3 4727 150 16 149 1445 1460 3647 271 4\nI0701 23:24:24.990008 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.990144 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.990255 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 2 12 22 26 33 40 42 44 45 50 58 72 85 98 100 101 102 113 118 0\nI0701 23:24:24.990357 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 350 1548 4831 3194 3287 2518 1344 914 457 5423 1528 328 16 149 218 150 16 149 149 0\nI0701 23:24:24.990463 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.990554 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.991022 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.991238 140630622037824 create_pretraining_data.py:151] tokens: [CLS] ##आईएफ म बदला जा सकता ह ) और कोल ##ा ##ज कर ##िएटर जसी सविधा ह । आह ##ा [MASK] यह पसद ह [MASK] 16 ##00 यह दग ##न स भी काफी अधिक ह कि तम [MASK] ax33 मिल [MASK] ह , तो [MASK] लिए आप जानत ह कि यह [MASK] कया आप खरच करना चाहत ह पर निर ##भर करता ह – youtube पर आप कया कर रह ह . कया तम [MASK] म य ##टय [SEP] previous post : ka ##reen ##a ka ##po ##or n ##ever do sh ##op ##p ##ing for [MASK] [MASK] [MASK] ##i kh ##an [MASK] detail ##s | shock ##ing : करी [MASK] कपर खान पति सफ क लिए ##मातमक नही करती शॉपिग [MASK] एकटर न [MASK] बताई वजह [SEP]\nI0701 23:24:24.991412 140630622037824 create_pretraining_data.py:161] input_ids: 2 5948 120 1695 351 689 128 13 322 3391 218 249 315 4496 3729 2626 128 136 2940 218 3 416 2254 128 3 949 1001 416 3281 215 127 348 770 697 128 326 1181 3 6212 505 3 128 16 377 3 350 353 1689 128 326 416 3 682 353 3164 563 1168 128 312 513 3848 814 128 147 2911 312 353 682 315 379 128 18 682 1181 3 120 121 3332 4 5723 1710 30 5149 2382 260 5149 3358 378 55 4434 1884 1235 1135 238 549 1120 3 3 3 231 1095 400 3 5037 244 69 4798 549 30 1941 3 1892 744 1906 2029 97 350 5631 357 1470 6094 3 2359 115 3 3472 1793 4\nI0701 23:24:24.991556 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.991916 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.992085 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 5 16 20 24 37 40 44 51 52 74 81 96 97 98 102 110 117 121 124 0\nI0701 23:24:24.992202 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 689 128 147 8 682 689 97 449 682 665 30 3681 5457 992 1851 320 1177 16 912 0\nI0701 23:24:24.992319 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.992415 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.992925 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.993187 140630622037824 create_pretraining_data.py:151] tokens: [CLS] क साथ - साथ [MASK] [MASK] man और कर ##र कदियो क लिए चाचा [MASK] [MASK] म मशहर ह . इस जल म कदी गग बनाकर रहत ह , जिसकी वजह स यहा कई बार खन ##ी गगवार होत ह . [MASK] 19 ##9 ##4 म जल म एक ऐसा ख ##ौफ ##नाक गगवार हआ कि [MASK] 100 स जयादा कदी मार [MASK] [SEP] म कमी आ रही ह ? jpy ##ना गपता व दिगगज करिकट खिलाडी [MASK] ##यन र ##ि ##चर ##डस की बटी मसाबा न परमार - हा , मझ ऐसा लगता ह । एक डिजाइन हा ##उस क रप म हमार पास [MASK] [MASK] ##द करन का जर ##िया ह [MASK] एक महिला होन क ना ##त , सीधा [MASK] राय ह , [SEP]\nI0701 23:24:24.993400 140630622037824 create_pretraining_data.py:161] input_ids: 2 97 386 17 386 3 3 2932 322 315 211 1984 97 350 6063 3 3 120 3153 128 18 329 692 120 2702 2954 3096 1768 128 16 4601 1793 127 746 704 460 4251 230 6243 1322 128 18 3 630 226 284 120 692 120 338 1123 98 5528 3050 6243 769 326 3 1543 127 891 2702 693 3 4 120 2065 85 567 128 35 6388 320 1217 124 5012 2051 1980 3 1457 122 212 1088 2039 316 2320 2278 115 4673 17 860 16 732 1123 1693 128 136 338 1873 860 3847 97 516 120 866 884 3 3 216 397 319 728 321 128 3 338 919 558 97 730 236 16 4606 3 1015 128 16 4\nI0701 23:24:24.993582 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.993765 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.993890 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 5 6 7 14 15 16 42 52 57 63 71 78 88 106 107 114 118 122 123 0\nI0701 23:24:24.994036 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 369 4614 4395 348 886 685 686 5528 3887 638 565 5600 426 2699 581 322 97 1102 338 0\nI0701 23:24:24.994171 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.994292 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.994772 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.995017 140630622037824 create_pretraining_data.py:151] tokens: [CLS] की टीम 17 ##2 ##6 [MASK] क साथ शीरष पर पहच गई ह । फरास को [MASK] [MASK] का फायदा हआ ह । फरास क अलावा फाइनल [MASK] पहचन वाली करोएशिया को सबस जयादा 16 अको का लाभ हआ ह । [MASK] बार का चपियन जरमनी 14 सथान [MASK] ल ##ढ ##क [MASK] 15 ##व नबर पर खिस ##क गया ह । सबस जयादा बार खिताब जीतन [MASK] अरजटीना छह सथान फि [MASK] ##कर शीरष सिदधात 10 [MASK] बाहर [SEP] करोएशिया अब 16 ##43 अको क साथ चौथ नबर पर पहच गई ह । [MASK] कप समी ##फा ##इन ##ल तक पहची बल ##जय ##िम [MASK] सथान ऊपर उठ ##कर दसर जबकि बराजील एक सथान नीच [MASK] [MASK] तीसर [MASK] पर खिस ##क गई ह । [SEP]\nI0701 23:24:24.995232 140630622037824 create_pretraining_data.py:161] input_ids: 2 316 1330 1079 281 282 3 97 386 3616 312 749 544 128 136 2146 317 3 3 319 2902 769 128 136 2146 97 1208 3298 3 3972 1070 2422 317 818 891 949 2694 319 1864 769 128 136 3 460 319 6332 3614 727 1074 3 123 263 207 3 937 208 1577 312 5230 207 413 128 136 818 891 460 2659 1959 3 4151 3724 1074 3755 3 340 3616 4939 507 3 1421 4 2422 496 949 5541 2694 97 386 3994 1577 312 749 544 128 136 3 766 2196 1835 1838 214 468 2397 581 589 561 3 1074 1890 1080 340 1343 1541 4038 338 1074 1779 3 3 4977 3 312 5230 207 544 128 136 4\nI0701 23:24:24.995416 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.995586 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.995708 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 6 7 8 17 18 28 42 49 53 59 68 73 76 78 95 106 117 118 120 0\nI0701 23:24:24.995832 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 2694 97 386 3724 2865 120 2105 1779 340 207 1070 677 17 127 981 338 2486 340 1577 0\nI0701 23:24:24.995979 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.996091 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\nI0701 23:24:24.996567 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.996791 140630622037824 create_pretraining_data.py:151] tokens: [CLS] [MASK] अब 16 ##43 अको क [MASK] चौथ [MASK] पर पहच गई ह । विशव कप समी ##फा ##इन ##ल तक पहची बल ##जय ##िम मिला सथान ऊपर उठ ##कर दसर जबकि बराजील एक सथान नीच गिर ##कर तीसर नबर पर खिस ##क गई अछि । [SEP] बयान दिया ह । उनहोन कहा कि राम मदिर बनगा [MASK] सवा ##भाव ##िक सबको . . . [MASK] ##ियो को 59 मिनट [MASK] एक [MASK] तक का लो ##न दन क लिए [MASK] भर म ##ाला ##स ##एम ##ई कारयकरम सर [MASK] किया गया । इस [MASK] क [MASK] स लोगो को जा ##गर ##क . . [MASK] उततर परदश क डिप [MASK] सीएम कशव परसाद मौरय न परव [MASK] मलायम सिह [MASK] पर जमकर निशाना साधा । [SEP]\nI0701 23:24:24.997004 140630622037824 create_pretraining_data.py:161] input_ids: 2 3 496 949 5541 2694 97 3 3994 3 312 749 544 128 136 981 766 2196 1835 1838 214 468 2397 581 589 561 1729 1074 1890 1080 340 1343 1541 4038 338 1074 1779 2486 340 4977 1577 312 5230 207 544 2658 136 4 1911 459 128 136 541 426 326 858 3503 5691 3 826 1167 323 3445 18 18 18 3 405 317 3220 2737 3 338 3 468 319 425 215 571 97 350 3 685 120 861 222 943 237 1073 387 3 393 413 136 329 3 97 3 127 663 317 351 406 207 18 18 3 1229 1002 97 3276 3 3087 4248 2405 6431 115 481 3 6168 501 3 312 3556 2432 4499 136 4\nI0701 23:24:24.997193 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.997381 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.997504 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 1 7 9 26 45 58 66 71 73 81 84 89 90 95 97 106 111 118 121 0\nI0701 23:24:24.997630 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 2422 386 1577 338 128 377 3537 120 2560 1002 5998 319 3561 1073 1795 18 418 1260 869 0\nI0701 23:24:24.997763 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.997875 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:24.998382 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:24.998610 140630622037824 create_pretraining_data.py:151] tokens: [CLS] . मोन सोचत होय ##ब [MASK] लगभग पाच [MASK] sab ##ि लोक ##क ठ ##ोर पर जा ##हि अम ##िट , अति ##पा ##वन [MASK] [MASK] राज अछि हम [MASK] भाषा भारत ##क सगहि न . . . बिहार सरकार ##क छवि मिथिला - मथिली विरोधी बन ##ल अछि . [MASK] कोनो एक दिन या एक साल म न भल ##ए . [MASK] [SEP] कारण अलाउदीन की नजर हममीर दव पर [MASK] गयी [MASK] रणथमभोर क किल पर धा [MASK] बोल दिया गया और [MASK] ##स नदी क किनार अलाउदीन और हममीर दव की सना क बीच [MASK] [MASK] जिसम राजपत सना विजयी हयी लकिन अचानक किसी तरह स [MASK] दव की सना बि मालिको गयी थी इसका कारण [MASK] सनापति गर ##दन [MASK] की परधान [SEP]\nI0701 23:24:24.998809 140630622037824 create_pretraining_data.py:161] input_ids: 2 18 5327 6143 5580 228 3 2224 1772 3 4205 212 1425 207 107 395 312 351 1131 570 603 16 1499 723 556 3 3 436 2658 388 3 1143 621 207 6035 115 18 18 18 1981 498 207 4271 4933 17 3184 2883 450 214 2658 18 3 4484 338 652 545 338 686 120 115 1579 262 18 3 4 963 1094 316 1183 3423 729 312 3 1707 3 6482 97 3396 312 3283 3 1910 459 413 322 3 222 4302 97 3397 1094 322 3423 729 316 957 97 933 3 3 1113 2579 957 4843 4050 509 2648 536 738 127 3 729 316 957 915 4012 1707 524 1413 963 3 2242 720 736 3 316 1163 4\nI0701 23:24:24.999004 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.999186 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:24.999314 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 6 9 10 25 26 29 30 51 63 72 74 80 85 98 99 110 115 120 124 0\nI0701 23:24:24.999456 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 103 317 209 1143 207 16 103 87 4933 848 69 355 560 4033 769 3423 2042 402 3321 0\nI0701 23:24:24.999595 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:24.999706 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 1\nI0701 23:24:25.000187 140630622037824 create_pretraining_data.py:149] *** Example ***\nI0701 23:24:25.000433 140630622037824 create_pretraining_data.py:151] tokens: [CLS] अजय दवगन और अकषय कमार की टककर काफी अजीब ह । दोनो क [MASK] [MASK] स एक अन [MASK] [MASK] सी [MASK] [MASK] [MASK] और अब य टककर एक बार नही दो दो बार ##बली जा रही ह । कारण ह काम आन वाली फिलम । [SEP] [MASK] सपोरटस बायोपिक क लिए हाथ मिला ##या ह जिस तव ##र क डायरकटर अमित शरमा डायर ##कट करग [MASK] सपोरटस बायोपिक स कछ द ##िमा ##ग की बत ##ती जल ##ी ? [MASK] , अकषय कमार [MASK] ##र गोलड पहल एक सपोरटस बायोपिक ही थी । लकिन फिर इस कवल एक इवट पर आधारित फिलम [MASK] दिया गया । भारत का पहला गोलड जीतन [MASK] इवट । [MASK] अब अजय दवगन भी [MASK] फिलम म दिलचसपी दिखा रह [SEP]\nI0701 23:24:25.000640 140630622037824 create_pretraining_data.py:161] input_ids: 2 1718 2850 322 2473 469 316 2886 770 6291 128 136 750 97 3 3 127 338 411 3 3 486 3 3 3 322 496 121 2886 338 460 357 479 479 460 5450 351 567 128 136 963 128 688 910 1070 650 136 4 3 3639 3207 97 350 1551 1729 313 128 472 4283 211 97 6333 3475 761 4282 958 1320 3 3639 3207 127 494 113 3335 257 316 2022 354 692 230 35 3 16 2473 469 3 211 3576 635 338 3639 3207 381 524 136 509 712 329 1333 338 2944 312 1628 650 3 459 413 136 621 319 2207 3576 1959 3 2944 136 3 496 1718 2850 348 3 650 120 6205 2241 379 4\nI0701 23:24:25.000825 140630622037824 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:25.001015 140630622037824 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nI0701 23:24:25.001147 140630622037824 create_pretraining_data.py:161] masked_lm_positions: 14 15 19 20 22 23 24 35 42 48 60 62 67 81 85 104 113 116 121 0\nI0701 23:24:25.001283 140630622037824 create_pretraining_data.py:161] masked_lm_ids: 933 1769 615 330 2886 567 128 558 990 338 97 3475 136 2210 5836 566 319 322 3639 0\nI0701 23:24:25.001419 140630622037824 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\nI0701 23:24:25.001529 140630622037824 create_pretraining_data.py:161] next_sentence_labels: 0\n","name":"stdout"},{"output_type":"stream","text":"I0701 23:24:25.747543 140630622037824 create_pretraining_data.py:166] Wrote 2561 total instances\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python run_pretraining.py \\\n    --input_file=gs://tf-lang-model_danzter/*.tfrecord \\\n    --output_dir=gs://tf-lang-model_danzter/model/ \\\n    --do_train=True \\\n    --do_eval=True \\\n    --bert_config_file=/kaggle/input/bert-base-uncased/config.json \\\n    --train_batch_size=32 \\\n    --max_seq_length=128 \\\n    --max_predictions_per_seq=20 \\\n    --num_train_steps=20 \\\n    --num_warmup_steps=10 \\\n    --learning_rate=2e-5 \\\n    --use_tpu=True \\\n    --tpu_name=$TPU_NAME","execution_count":10,"outputs":[{"output_type":"stream","text":"W0701 23:24:30.052810 140205107017536 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n\nW0701 23:24:30.053118 140205107017536 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n\nW0701 23:24:30.053412 140205107017536 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\nW0701 23:24:30.055700 140205107017536 module_wrapper.py:139] From run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n\n2020-07-01 23:24:30.056107: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nW0701 23:24:30.116468 140205107017536 module_wrapper.py:139] From run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n\n2020-07-01 23:24:30.116882: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nW0701 23:24:30.159310 140205107017536 module_wrapper.py:139] From run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n\nI0701 23:24:30.159664 140205107017536 run_pretraining.py:420] *** Input Files ***\nI0701 23:24:30.159823 140205107017536 run_pretraining.py:422]   gs://tf-lang-model_danzter/tf_examples.tfrecord\nW0701 23:24:30.160066 140205107017536 lazy_loader.py:50] \nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nI0701 23:24:31.680767 140205107017536 utils.py:141] NumExpr defaulting to 4 threads.\nW0701 23:24:32.427338 140205107017536 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f83a33a14d0>) includes params argument, but params are not passed to Estimator.\nI0701 23:24:32.428285 140205107017536 estimator.py:212] Using config: {'_model_dir': 'gs://tf-lang-model_danzter/model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\ncluster_def {\n  job {\n    name: \"worker\"\n    tasks {\n      key: 0\n      value: \"10.0.0.2:8470\"\n    }\n  }\n}\nisolate_session_state: true\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83a335ec90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.0.0.2:8470', '_evaluation_master': 'grpc://10.0.0.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f83bb8f68d0>}\nI0701 23:24:32.428764 140205107017536 tpu_context.py:220] _TPUContext: eval_on_tpu True\nI0701 23:24:32.429054 140205107017536 run_pretraining.py:459] ***** Running training *****\nI0701 23:24:32.429183 140205107017536 run_pretraining.py:460]   Batch size = 32\n2020-07-01 23:24:32.429817: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.518517: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.646770: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.684108: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.728996: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.814536: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.912942: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:32.967723: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:33.071473: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nI0701 23:24:33.229618 140205107017536 estimator.py:363] Skipping training since max_steps has already saved.\nI0701 23:24:33.229988 140205107017536 error_handling.py:101] training_loop marked as finished\nI0701 23:24:33.230234 140205107017536 run_pretraining.py:469] ***** Running evaluation *****\nI0701 23:24:33.230396 140205107017536 run_pretraining.py:470]   Batch size = 8\nI0701 23:24:33.230746 140205107017536 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.0.0.2:8470) for TPU system metadata.\n2020-07-01 23:24:33.232159: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\nI0701 23:24:33.236516 140205107017536 tpu_system_metadata.py:148] Found TPU system:\nI0701 23:24:33.236792 140205107017536 tpu_system_metadata.py:149] *** Num TPU Cores: 8\nI0701 23:24:33.236984 140205107017536 tpu_system_metadata.py:150] *** Num TPU Workers: 1\nI0701 23:24:33.237153 140205107017536 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\nI0701 23:24:33.237280 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10791054535373007051)\nI0701 23:24:33.237641 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10884162682109530155)\nI0701 23:24:33.237978 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15299783932267063908)\nI0701 23:24:33.238153 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5479659579104651560)\nI0701 23:24:33.238343 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9821338810459103183)\nI0701 23:24:33.238473 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6899304363520189745)\nI0701 23:24:33.238611 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1132009937123157756)\nI0701 23:24:33.238728 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17203540251797340129)\nI0701 23:24:33.238868 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16474093082194468983)\nI0701 23:24:33.238994 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 7165259738385266281)\nI0701 23:24:33.239127 140205107017536 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8906320047173141700)\n","name":"stdout"},{"output_type":"stream","text":"2020-07-01 23:24:33.283509: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:24:33.302474: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nW0701 23:24:33.349067 140205107017536 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nI0701 23:24:33.350560 140205107017536 estimator.py:1148] Calling model_fn.\nW0701 23:24:33.351366 140205107017536 module_wrapper.py:139] From run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n\nW0701 23:24:33.374083 140205107017536 deprecation.py:323] From run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.experimental.map_and_batch(...)`.\nW0701 23:24:33.374398 140205107017536 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\nW0701 23:24:33.478734 140205107017536 module_wrapper.py:139] From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n\nW0701 23:24:33.655589 140205107017536 deprecation.py:323] From run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nI0701 23:24:33.694428 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.696747 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.699010 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.701252 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.703881 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.706430 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.709267 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\nI0701 23:24:33.712036 140205107017536 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\n2020-07-01 23:24:33.736310: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2020-07-01 23:24:33.736378: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2020-07-01 23:24:33.736406: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (1006753d5d22): /proc/driver/nvidia/version does not exist\nI0701 23:24:33.758917 140205107017536 run_pretraining.py:117] *** Features ***\nI0701 23:24:33.759282 140205107017536 run_pretraining.py:119]   name = input_ids, shape = (1, 128)\nI0701 23:24:33.759404 140205107017536 run_pretraining.py:119]   name = input_mask, shape = (1, 128)\nI0701 23:24:33.759506 140205107017536 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\nI0701 23:24:33.759601 140205107017536 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\nI0701 23:24:33.759698 140205107017536 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\nI0701 23:24:33.759792 140205107017536 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\nI0701 23:24:33.759884 140205107017536 run_pretraining.py:119]   name = segment_ids, shape = (1, 128)\nW0701 23:24:33.760176 140205107017536 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nW0701 23:24:33.762689 140205107017536 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nW0701 23:24:33.802286 140205107017536 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n\nW0701 23:24:33.869182 140205107017536 deprecation.py:323] From /kaggle/input/bertsrc/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.Dense instead.\nW0701 23:24:33.870404 140205107017536 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nW0701 23:24:36.875447 140205107017536 module_wrapper.py:139] From run_pretraining.py:150: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n\nI0701 23:24:36.875789 140205107017536 run_pretraining.py:167] **** Trainable Variables ****\nI0701 23:24:36.875943 140205107017536 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768)\nI0701 23:24:36.876123 140205107017536 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\nI0701 23:24:36.876274 140205107017536 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\nI0701 23:24:36.876412 140205107017536 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.876544 140205107017536 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.876674 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.876811 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.876971 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.877111 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.877250 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.877383 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.877511 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.877642 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.877782 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.877908 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.878054 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.878196 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.878324 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.878459 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.878586 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.878716 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.878844 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.878995 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.879122 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.879262 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.879392 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.879525 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.879651 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.879787 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.879917 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.880059 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.880196 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.880334 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.880463 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.880597 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.880729 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.880857 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.881000 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.881144 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.881289 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.881424 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.881554 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.881689 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.881819 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.881978 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.882114 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.882254 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.882390 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.882542 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.882678 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.882816 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.882944 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.883095 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.883233 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.883372 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.883501 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.883633 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.883764 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.883897 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.884045 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.884186 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.884316 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.884443 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.884569 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.884698 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.884822 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.884968 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.885094 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.885226 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.885354 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.885485 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.885619 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.885753 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.885855 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.885973 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.886088 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.886201 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.886300 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.886421 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.886519 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.886623 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.886721 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.886824 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.886921 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.887035 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.887142 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.887247 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.887344 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.887445 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.887543 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.887645 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.887744 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.887847 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.887944 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.888058 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.888162 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.888266 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.888363 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.888466 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.888564 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.888660 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.888796 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.888909 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.889023 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.889136 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.889236 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.889339 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.889436 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.889539 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.889637 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.889733 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.889831 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.889941 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.890094 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\nI0701 23:24:36.890228 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.890351 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.890473 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.890598 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\nI0701 23:24:36.890730 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\nI0701 23:24:36.890858 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\nI0701 23:24:36.891004 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\nI0701 23:24:36.891142 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\nI0701 23:24:36.891278 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\nI0701 23:24:36.891411 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\nI0701 23:24:36.891544 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\nI0701 23:24:36.891669 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\nI0701 23:24:36.891791 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\nI0701 23:24:36.891914 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\nI0701 23:24:36.892061 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\nI0701 23:24:36.892194 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n","name":"stdout"},{"output_type":"stream","text":"I0701 23:24:36.892337 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.892462 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.892585 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.892710 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.892858 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\r\nI0701 23:24:36.892999 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.893168 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\r\nI0701 23:24:36.893296 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.893429 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\r\nI0701 23:24:36.893554 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.893683 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.893807 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.893932 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.894074 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\r\nI0701 23:24:36.894213 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\r\nI0701 23:24:36.894339 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\r\nI0701 23:24:36.894512 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.894671 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.894831 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.894992 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.895160 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\r\nI0701 23:24:36.895314 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.895476 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\r\nI0701 23:24:36.895631 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.895800 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\r\nI0701 23:24:36.895992 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.896185 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.896356 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.896512 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.896672 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\r\nI0701 23:24:36.896852 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\r\nI0701 23:24:36.897037 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\r\nI0701 23:24:36.897232 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.897392 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.897538 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.897690 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.897877 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\r\nI0701 23:24:36.898061 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.898241 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\r\nI0701 23:24:36.898354 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.898509 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\r\nI0701 23:24:36.898611 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.898758 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.898859 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.898974 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.899079 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\r\nI0701 23:24:36.899195 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\r\nI0701 23:24:36.899301 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\r\nI0701 23:24:36.899405 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.899502 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.899600 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.899722 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.899831 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\r\nI0701 23:24:36.899933 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.900073 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\r\nI0701 23:24:36.900184 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.900299 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\r\nI0701 23:24:36.900399 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.900528 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.900646 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.901006 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.901206 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\r\nI0701 23:24:36.901370 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\r\nI0701 23:24:36.901506 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\r\nI0701 23:24:36.901662 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.901801 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.901936 140205107017536 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.902127 140205107017536 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.902289 140205107017536 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.902436 140205107017536 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\r\nI0701 23:24:36.902582 140205107017536 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\r\nI0701 23:24:36.902796 140205107017536 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\r\nI0701 23:24:36.902939 140205107017536 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\r\nI0701 23:24:36.903135 140205107017536 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30522,)\r\nI0701 23:24:36.903419 140205107017536 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\r\nI0701 23:24:36.903591 140205107017536 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\r\nW0701 23:24:36.919247 140205107017536 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\n","name":"stdout"},{"output_type":"stream","text":"W0701 23:24:37.346451 140205107017536 module_wrapper.py:139] From run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n\nW0701 23:24:37.364413 140205107017536 module_wrapper.py:139] From run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n\nI0701 23:24:37.425478 140205107017536 estimator.py:1150] Done calling model_fn.\nI0701 23:24:37.446460 140205107017536 evaluation.py:255] Starting evaluation at 2020-07-01T23:24:37Z\nI0701 23:24:37.446757 140205107017536 tpu_estimator.py:506] TPU job name worker\nW0701 23:24:37.585012 140205107017536 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nI0701 23:24:38.012066 140205107017536 monitored_session.py:240] Graph was finalized.\n2020-07-01 23:24:38.013042: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nI0701 23:24:38.056734 140205107017536 saver.py:1284] Restoring parameters from gs://tf-lang-model_danzter/model/model.ckpt-20\nI0701 23:24:49.897306 140205107017536 session_manager.py:500] Running local_init_op.\nI0701 23:24:49.987393 140205107017536 session_manager.py:502] Done running local_init_op.\nW0701 23:24:50.143004 140205107017536 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:802: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nPrefer Variable.assign which has equivalent behavior in 2.X.\nI0701 23:24:50.234592 140205107017536 tpu_estimator.py:567] Init TPU system\nI0701 23:24:54.032822 140205107017536 tpu_estimator.py:576] Initialized TPU in 3 seconds\nI0701 23:24:54.033778 140203070310144 tpu_estimator.py:521] Starting infeed thread controller.\nI0701 23:24:54.034500 140203061917440 tpu_estimator.py:540] Starting outfeed thread controller.\nI0701 23:24:54.141581 140205107017536 util.py:98] Initialized dataset iterators in 0 seconds\nI0701 23:24:54.238171 140205107017536 tpu_estimator.py:600] Enqueue next (100) batch(es) of data to infeed.\nI0701 23:24:54.238695 140205107017536 tpu_estimator.py:604] Dequeue next (100) batch(es) of data from outfeed.\nI0701 23:24:57.332221 140203061917440 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\nI0701 23:24:59.293130 140205107017536 evaluation.py:167] Evaluation [100/100]\nI0701 23:24:59.293542 140205107017536 tpu_estimator.py:608] Stop infeed thread controller\nI0701 23:24:59.293782 140205107017536 tpu_estimator.py:434] Shutting down InfeedController thread.\nI0701 23:24:59.294097 140203070310144 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\nI0701 23:24:59.294329 140203070310144 tpu_estimator.py:537] Infeed thread finished, shutting down.\nI0701 23:24:59.294572 140205107017536 error_handling.py:101] infeed marked as finished\nI0701 23:24:59.294787 140205107017536 tpu_estimator.py:612] Stop output thread controller\nI0701 23:24:59.294901 140205107017536 tpu_estimator.py:434] Shutting down OutfeedController thread.\nI0701 23:24:59.916796 140203061917440 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\nI0701 23:24:59.917189 140203061917440 tpu_estimator.py:551] Outfeed thread finished, shutting down.\nI0701 23:24:59.917423 140205107017536 error_handling.py:101] outfeed marked as finished\nI0701 23:24:59.917630 140205107017536 tpu_estimator.py:616] Shutdown TPU system.\nI0701 23:25:00.230921 140205107017536 evaluation.py:275] Finished evaluation at 2020-07-01-23:25:00\nI0701 23:25:00.231334 140205107017536 estimator.py:2049] Saving dict for global step 20: global_step = 20, loss = 9.955372, masked_lm_accuracy = 0.027732033, masked_lm_loss = 9.28602, next_sentence_accuracy = 0.55625, next_sentence_loss = 0.6846589\n2020-07-01 23:25:00.231607: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:00.537216: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:00.603977: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:00.864668: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:00.902021: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:00.985587: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:01.264548: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:01.585850: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:01.669862: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:02.004257: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n","name":"stdout"},{"output_type":"stream","text":"I0701 23:25:02.038308 140205107017536 estimator.py:2109] Saving 'checkpoint_path' summary for global step 20: gs://tf-lang-model_danzter/model/model.ckpt-20\n2020-07-01 23:25:02.039350: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:02.108354: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:02.604110: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\nI0701 23:25:02.653210 140205107017536 error_handling.py:101] evaluation_loop marked as finished\nI0701 23:25:02.653610 140205107017536 run_pretraining.py:483] ***** Eval results *****\nI0701 23:25:02.653743 140205107017536 run_pretraining.py:485]   global_step = 20\nI0701 23:25:02.654154 140205107017536 run_pretraining.py:485]   loss = 9.955372\nI0701 23:25:02.654347 140205107017536 run_pretraining.py:485]   masked_lm_accuracy = 0.027732033\nI0701 23:25:02.654452 140205107017536 run_pretraining.py:485]   masked_lm_loss = 9.28602\nI0701 23:25:02.654545 140205107017536 run_pretraining.py:485]   next_sentence_accuracy = 0.55625\nI0701 23:25:02.654636 140205107017536 run_pretraining.py:485]   next_sentence_loss = 0.6846589\n2020-07-01 23:25:02.654807: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n2020-07-01 23:25:02.726552: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Cancelled: GCE check skipped due to presence of $NO_GCE_CHECK environment variable.\".\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}